{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.install_pypi_package('pandas==1.3.5')\n",
    "sc.install_pypi_package('boto3')\n",
    "sc.install_pypi_package('pyspark==2.3.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re\n",
    "import tempfile\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import subprocess\n",
    "from pyspark.sql.functions import udf, col, lit\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\"\"\"\n",
    "Import Parquet As a DataFrame\n",
    "\"\"\"\n",
    "\n",
    "#Read in parquet file from public S3 bucket\n",
    "parquet_s3 = \"s3://macaquenaive/parquet/\"\n",
    "df_spark = spark.read.parquet(parquet_s3)\n",
    "\n",
    "##Verify count \n",
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This cell filter out sequences from animals that have less than 100,000 IgMs, in addition to animals RPz18, RGp18, RPb18, and REt18.\n",
    "## For dataset of animals RPz18, RGp18, RPb18, and REt18, only IgMs were sorted and sequenced, but some of their c_call reads were incomplete and therefore not reliable.\n",
    "## To address this issue, we analyzed precursor frequencies of these four animals separately from the next cell.\n",
    "df_spark_100000 = df_spark[~df_spark[\"animal_id\"].isin([\"4440\",\"2H2\",\"8G5\",\"OH7\",\"RBg12\",\"Rh-A13001\",\"Rh-A13002\",\"Rh-A13003\",\"Rh-A13004\",\"Rh-A13006\",\"Rh-A13007\",\"Rh-A13009\",\"Rh-A13012\",\"Rh-A13013\",\"RPz18\",\"RGp18\",\"RPb18\",\"REt18\"])]\n",
    "    \n",
    "df_spark_100000.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This cell searches for IgM precursors and calculate the IgM precursor frequencies of all animals with c_call=\"IGHM*03\".\n",
    "## However, for dataset of animals RPz18, RGp18, RPb18, and REt18, only IgMs were sorted and sequenced, and some of their c_call reads were incomplete and therefore not reliable.\n",
    "## To address this issue, we used the next cell to analyze precursor frequencies of these four animals separately from this cell.\n",
    "\n",
    "class Query():\n",
    "    \n",
    "    '''An example query class to hold query parameters'''\n",
    "    \n",
    "    def __init__(self,q_name,length='',d_call_top=\"\",regex=\"\",animal_id=\"\",c_call=\"\",file_name=\"\"):\n",
    "        self.query_name = q_name\n",
    "        self.d_call_top = d_call_top\n",
    "        self.animal_id = animal_id\n",
    "        self.c_call = c_call\n",
    "        self.file_name = file_name\n",
    "        \n",
    "        if not length:\n",
    "            raise Exception(\"Length must be supplied\")\n",
    "        self.length = length\n",
    "        self.regular_expression = regex\n",
    "    \n",
    "    \n",
    "    \n",
    "    def apply(self,df):\n",
    "        \n",
    "        '''Apply function will take in spark dataframe and apply query parameters to it if they exist\n",
    "        \n",
    "           Returns a filtered dataframe\n",
    "        '''\n",
    "        self.queried_dataframe = \"\"\n",
    "        \n",
    "        ##Lets get length\n",
    "        \n",
    "        self.queried_dataframe = df.filter(F.length(df.cdr3_aa) > self.length)\n",
    "        \n",
    "        ##If the rest of these were specified, add them to the filter\n",
    "                   \n",
    "        if self.d_call_top:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.d_call_top.rlike(self.d_call_top))\n",
    "                \n",
    "        if self.c_call:\n",
    "            self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.c_call == self.c_call)      \n",
    "\n",
    "        if self.regular_expression:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.cdr3_aa.rlike(self.regular_expression))\n",
    "        \n",
    "        if self.animal_id:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.animal_id == self.animal_id)\n",
    "                \n",
    "        if self.file_name:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.file_name.rlike(self.file_name))\n",
    "                \n",
    "        print(\"Found {} sequences\".format(self.queried_dataframe.count()))\n",
    "        return self.queried_dataframe\n",
    "        \n",
    "    \n",
    "def write_out_hadoop_to_local(df, s3):\n",
    "    '''Because spark is distributed, writing out results is done by each worker node. As a result, it breaks up the results into partions. \n",
    "    \n",
    "    This function will search the hadoop file system and concatenate the parted file back to a single CSV file\n",
    "    \n",
    "    params df - the quried dataframe\n",
    "    s3 - the local file path (make sure its chmod 777)\n",
    "    '''\n",
    "    \n",
    "    #Output Name\n",
    "    output_s3 = s3\n",
    "    \n",
    "    #A temporary file to write out Hadoop CSV\n",
    "    t = tempfile.NamedTemporaryFile(delete=False)\n",
    "    \n",
    "\n",
    "    #Write out the CSV\n",
    "    df.write.csv(t.name,mode='overwrite')\n",
    "    with open(output_s3,'w') as f:\n",
    "        f.write(\",\".join(df.columns)+'\\n')\n",
    "        \n",
    "        print(\"Running HDFS concatenate\\n{} > {}\".format(' '.join(['hdfs','dfs','-cat','{}/*csv'.format(t.name)]),output_s3))\n",
    "        subprocess.Popen(['hdfs','dfs','-cat','{}/*csv'.format(t.name)],stdout=f)\n",
    "        \n",
    "\n",
    "    print('Wrote CSV to {}'.format(output_s3))\n",
    "my_query_0 = Query('BG18_search.0',length=1,c_call=\"IGHM*03\")\n",
    "my_query_1 = Query('BG18_search.1',length=21,c_call=\"IGHM*03\")\n",
    "my_query_2 = Query('BG18_search.2',d_call_top=r'IGHD3-41',length=21,c_call=\"IGHM*03\")\n",
    "my_query_3 = Query('BG18_search.3',d_call_top=r'IGHD3-41',length=21,regex=r'IFG[VL]',c_call=\"IGHM*03\")\n",
    "my_query_4_7 = Query('BG18_search.4.7',d_call_top=r'IGHD3-41',length=21,regex=r'^.....IFG[VL]',c_call=\"IGHM*03\")\n",
    "my_query_4_8 = Query('BG18_search.4.8',d_call_top=r'IGHD3-41',length=21,regex=r'^......IFG[VL]',c_call=\"IGHM*03\")\n",
    "my_query_4_9 = Query('BG18_search.4.9',d_call_top=r'IGHD3-41',length=21,regex=r'^.......IFG[VL]',c_call=\"IGHM*03\")\n",
    "my_query_5_7 = Query('BG18_search.5.7',d_call_top=r'IGHD3-41',length=21,regex=r'^.....IFG[VL]....E',c_call=\"IGHM*03\")\n",
    "my_query_5_8 = Query('BG18_search.5.8',d_call_top=r'IGHD3-41',length=21,regex=r'^......IFG[VL]....E',c_call=\"IGHM*03\")\n",
    "my_query_5_9 = Query('BG18_search.5.9',d_call_top=r'IGHD3-41',length=21,regex=r'^.......IFG[VL]....E',c_call=\"IGHM*03\")\n",
    "my_query_6 = Query('BG18_search.6',d_call_top=r'IGHD3-41',length=21, regex=r'IFG[VL]....E',c_call=\"IGHM*03\")\n",
    "my_query_7 = Query('BG18_search.7',d_call_top=r'IGHD3-41',length=1,c_call=\"IGHM*03\")\n",
    "\n",
    "#To run query, pass the input object from above to apply\n",
    "\n",
    "queried0_df = my_query_0.apply(df_spark_100000)\n",
    "queried1_df = my_query_1.apply(df_spark_100000)\n",
    "queried2_df = my_query_2.apply(df_spark_100000)\n",
    "queried3_df = my_query_3.apply(df_spark_100000)\n",
    "queried4_7_df = my_query_4_7.apply(df_spark_100000)\n",
    "queried4_8_df = my_query_4_8.apply(df_spark_100000)\n",
    "queried4_9_df = my_query_4_9.apply(df_spark_100000)\n",
    "queried5_7_df = my_query_5_7.apply(df_spark_100000)\n",
    "queried5_8_df = my_query_5_8.apply(df_spark_100000)\n",
    "queried5_9_df = my_query_5_9.apply(df_spark_100000)\n",
    "queried6_df = my_query_6.apply(df_spark_100000)\n",
    "queried7_df = my_query_7.apply(df_spark_100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This cell searches for precursors and calculate the precursor frequencies of RPz18, RGp18, RPb18, and REt18 only.\n",
    "## The dataset of animals RPz18, RGp18, RPb18, and REt18, only IgMs were sorted and sequenced, and some of their c_call reads were incomplete and therefore not reliable.\n",
    "## To address this issue, we used this cell to analyze precursor frequencies of these four animals separately from the previous cell.\n",
    "\n",
    "class Query():\n",
    "    \n",
    "    '''An example query class to hold query parameters'''\n",
    "    \n",
    "    def __init__(self,q_name,length='',d_call_top=\"\",regex=\"\",animal_id=\"\",c_call=\"\",file_name=\"\"):\n",
    "        self.query_name = q_name\n",
    "        self.d_call_top = d_call_top\n",
    "        self.animal_id = animal_id\n",
    "        self.c_call = c_call\n",
    "        self.file_name = file_name\n",
    "        \n",
    "        if not length:\n",
    "            raise Exception(\"Length must be supplied\")\n",
    "        self.length = length\n",
    "        self.regular_expression = regex\n",
    "    \n",
    "    \n",
    "    \n",
    "    def apply(self,df):\n",
    "        \n",
    "        '''Apply function will take in spark dataframe and apply query parameters to it if they exist\n",
    "        \n",
    "           Returns a filtered dataframe\n",
    "        '''\n",
    "        self.queried_dataframe = \"\"\n",
    "        \n",
    "        ##Lets get length\n",
    "        \n",
    "        self.queried_dataframe = df.filter(F.length(df.cdr3_aa) > self.length)\n",
    "        \n",
    "        ##If the rest of these were specified, add them to the filter\n",
    "                   \n",
    "        if self.d_call_top:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.d_call_top.rlike(self.d_call_top))\n",
    "                \n",
    "        if self.c_call:\n",
    "            self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.c_call == self.c_call)      \n",
    "\n",
    "        if self.regular_expression:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.cdr3_aa.rlike(self.regular_expression))\n",
    "        \n",
    "        if self.animal_id:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.animal_id == self.animal_id)\n",
    "                \n",
    "        if self.file_name:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.file_name.rlike(self.file_name))\n",
    "                \n",
    "        print(\"Found {} sequences\".format(self.queried_dataframe.count()))\n",
    "        return self.queried_dataframe\n",
    "        \n",
    "    \n",
    "def write_out_hadoop_to_local(df, s3):\n",
    "    '''Because spark is distributed, writing out results is done by each worker node. As a result, it breaks up the results into partions. \n",
    "    \n",
    "    This function will search the hadoop file system and concatenate the parted file back to a single CSV file\n",
    "    \n",
    "    params df - the quried dataframe\n",
    "    s3 - the local file path (make sure its chmod 777)\n",
    "    '''\n",
    "    \n",
    "    #Output Name\n",
    "    output_s3 = s3\n",
    "    \n",
    "    #A temporary file to write out Hadoop CSV\n",
    "    t = tempfile.NamedTemporaryFile(delete=False)\n",
    "    \n",
    "\n",
    "    #Write out the CSV\n",
    "    df.write.csv(t.name,mode='overwrite')\n",
    "    with open(output_s3,'w') as f:\n",
    "        f.write(\",\".join(df.columns)+'\\n')\n",
    "        \n",
    "        print(\"Running HDFS concatenate\\n{} > {}\".format(' '.join(['hdfs','dfs','-cat','{}/*csv'.format(t.name)]),output_s3))\n",
    "        subprocess.Popen(['hdfs','dfs','-cat','{}/*csv'.format(t.name)],stdout=f)\n",
    "        \n",
    "\n",
    "    \n",
    "    print('Wrote CSV to {}'.format(output_s3))\n",
    "my_query_0 = Query('BG18_search.0',length=1,file_name=r'_Functional')\n",
    "my_query_1 = Query('BG18_search.1',length=21,file_name=r'_Functional')\n",
    "my_query_2 = Query('BG18_search.2',d_call_top=r'IGHD3-41',length=21,file_name=r'_Functional')\n",
    "my_query_3 = Query('BG18_search.3',d_call_top=r'IGHD3-41',length=21,regex=r'IFG[VL]',file_name=r'_Functional')\n",
    "my_query_4_7 = Query('BG18_search.4.7',d_call_top=r'IGHD3-41',length=21,regex=r'^.....IFG[VL]',file_name=r'_Functional')\n",
    "my_query_4_8 = Query('BG18_search.4.8',d_call_top=r'IGHD3-41',length=21,regex=r'^......IFG[VL]',file_name=r'_Functional')\n",
    "my_query_4_9 = Query('BG18_search.4.9',d_call_top=r'IGHD3-41',length=21,regex=r'^.......IFG[VL]',file_name=r'_Functional')\n",
    "my_query_5_7 = Query('BG18_search.5.7',d_call_top=r'IGHD3-41',length=21,regex=r'^.....IFG[VL]....E',file_name=r'_Functional')\n",
    "my_query_5_8 = Query('BG18_search.5.8',d_call_top=r'IGHD3-41',length=21,regex=r'^......IFG[VL]....E',file_name=r'_Functional')\n",
    "my_query_5_9 = Query('BG18_search.5.9',d_call_top=r'IGHD3-41',length=21,regex=r'^.......IFG[VL]....E',file_name=r'_Functional')\n",
    "my_query_6 = Query('BG18_search.6',d_call_top=r'IGHD3-41',length=21, regex=r'IFG[VL]....E',file_name=r'_Functional')\n",
    "my_query_7 = Query('BG18_search.7',d_call_top=r'IGHD3-41',length=1,file_name=r'_Functional')\n",
    "\n",
    "#To run query, pass the input object from above to apply\n",
    "\n",
    "queried0_df = my_query_0.apply(df_spark)\n",
    "queried1_df = my_query_1.apply(df_spark)\n",
    "queried2_df = my_query_2.apply(df_spark)\n",
    "queried3_df = my_query_3.apply(df_spark)\n",
    "queried4_7_df = my_query_4_7.apply(df_spark)\n",
    "queried4_8_df = my_query_4_8.apply(df_spark)\n",
    "queried4_9_df = my_query_4_9.apply(df_spark)\n",
    "queried5_7_df = my_query_5_7.apply(df_spark)\n",
    "queried5_8_df = my_query_5_8.apply(df_spark)\n",
    "queried5_9_df = my_query_5_9.apply(df_spark)\n",
    "queried6_df = my_query_6.apply(df_spark)\n",
    "queried7_df = my_query_7.apply(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pandas0_df = queried0_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas1_df = queried1_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas2_df = queried2_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas3_df = queried3_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas47_df = queried4_7_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas48_df = queried4_8_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas49_df = queried4_9_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas57_df = queried5_7_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas58_df = queried5_8_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas59_df = queried5_9_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas6_df = queried6_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas7_df = queried7_df.select('sequence_id','animal_id').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas4_df = pandas.concat([pandas47_df,pandas48_df,pandas49_df])\n",
    "pandas5_df = pandas.concat([pandas57_df,pandas58_df,pandas59_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print out the number of IgM sequences from all animals  \n",
    "counts = pandas0_df.groupby('animal_id').count().rename({'sequence_id':'count'},axis=1).sort_values('count')\n",
    "pandas.set_option('display.max_rows', None)  \n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the number of IgM sequences from all animals in search 1  \n",
    "counts = pandas1_df.groupby('animal_id').count().rename({'sequence_id':'count'},axis=1).sort_values('count')\n",
    "pandas.set_option('display.max_rows', None)  \n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the number of IgM sequences from all animals in search 2  \n",
    "counts = pandas2_df.groupby('animal_id').count().rename({'sequence_id':'count'},axis=1).sort_values('count')\n",
    "pandas.set_option('display.max_rows', None)  \n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the number of IgM sequences from all animals in search 3  \n",
    "counts = pandas3_df.groupby('animal_id').count().rename({'sequence_id':'count'},axis=1).sort_values('count')\n",
    "pandas.set_option('display.max_rows', None)  \n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the number of IgM sequences from all animals in search 4 \n",
    "counts = pandas4_df.groupby('animal_id').count().rename({'sequence_id':'count'},axis=1).sort_values('count')\n",
    "pandas.set_option('display.max_rows', None)  \n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the number of IgM sequences from all animals in search 5  \n",
    "counts = pandas5_df.groupby('animal_id').count().rename({'sequence_id':'count'},axis=1).sort_values('count')\n",
    "pandas.set_option('display.max_rows', None)  \n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print out the number of IgM sequences from all animals in search 6  \n",
    "counts = pandas6_df.groupby('animal_id').count().rename({'sequence_id':'count'},axis=1).sort_values('count')\n",
    "pandas.set_option('display.max_rows', None)  \n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print out the number of IgM sequences from all animals in search 7  \n",
    "counts = pandas7_df.groupby('animal_id').count().rename({'sequence_id':'count'},axis=1).sort_values('count')\n",
    "pandas.set_option('display.max_rows', None)  \n",
    "print(counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
