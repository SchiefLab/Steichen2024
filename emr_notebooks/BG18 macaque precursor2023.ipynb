{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-06-20T23:20:59.683259Z",
     "iopub.status.busy": "2023-06-20T23:20:59.682910Z",
     "iopub.status.idle": "2023-06-20T23:22:05.708682Z",
     "shell.execute_reply": "2023-06-20T23:22:05.707942Z",
     "shell.execute_reply.started": "2023-06-20T23:20:59.683231Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4807fc0d3834fb5a9f7708118498a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1687300297410_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-37-126.us-west-2.compute.internal:20888/proxy/application_1687300297410_0002/\" class=\"emr-proxy-link j-2J5FQLC0SEOTU application_1687300297410_0002\" emr-resource=\"j-2J5FQLC0SEOTU\n\" application-id=\"application_1687300297410_0002\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-43-73.us-west-2.compute.internal:8042/node/containerlogs/container_1687300297410_0002_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.3.5\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /usr/local/lib64/python3.7/site-packages (from pandas==1.3.5) (1.20.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/site-packages (from pandas==1.3.5) (2022.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.3.5) (1.13.0)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-1.3.5 python-dateutil-2.8.2\n",
      "\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.26.157-py3-none-any.whl (135 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3) (1.0.1)\n",
      "Collecting botocore<1.30.0,>=1.29.157\n",
      "  Downloading botocore-1.29.157-py3-none-any.whl (10.9 MB)\n",
      "Collecting urllib3<1.27,>=1.25.4\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./tmp/1687303293099-0/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.157->boto3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.157->boto3) (1.13.0)\n",
      "Installing collected packages: urllib3, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.26.157 botocore-1.29.157 s3transfer-0.6.1 urllib3-1.26.16\n",
      "\n",
      "Collecting pyspark==2.3.4\n",
      "  Downloading pyspark-2.3.4.tar.gz (212.3 MB)\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: /mnt/yarn/usercache/livy/appcache/application_1687300297410_0002/container_1687300297410_0002_01_000001/tmp/1687303293099-0/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/mnt/tmp/pip-install-vtm0utc7/pyspark/setup.py'\"'\"'; __file__='\"'\"'/mnt/tmp/pip-install-vtm0utc7/pyspark/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /mnt/tmp/pip-pip-egg-info-8a6pb3mr\n",
      "         cwd: /mnt/tmp/pip-install-vtm0utc7/pyspark/\n",
      "    Complete output (24 lines):\n",
      "    Could not import pypandoc - required to package PySpark\n",
      "    Couldn't find index page for 'pypandoc' (maybe misspelled?)\n",
      "    No local packages or working download links found for pypandoc\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/mnt/tmp/pip-install-vtm0utc7/pyspark/setup.py\", line 224, in <module>\n",
      "        'Programming Language :: Python :: Implementation :: PyPy']\n",
      "      File \"/usr/lib64/python3.7/distutils/core.py\", line 108, in setup\n",
      "        _setup_distribution = dist = klass(attrs)\n",
      "      File \"/mnt/yarn/usercache/livy/appcache/application_1687300297410_0002/container_1687300297410_0002_01_000001/tmp/1687303293099-0/lib/python3.7/site-packages/setuptools/dist.py\", line 315, in __init__\n",
      "        self.fetch_build_eggs(attrs['setup_requires'])\n",
      "      File \"/mnt/yarn/usercache/livy/appcache/application_1687300297410_0002/container_1687300297410_0002_01_000001/tmp/1687303293099-0/lib/python3.7/site-packages/setuptools/dist.py\", line 361, in fetch_build_eggs\n",
      "        replace_conflicting=True,\n",
      "      File \"/mnt/yarn/usercache/livy/appcache/application_1687300297410_0002/container_1687300297410_0002_01_000001/tmp/1687303293099-0/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 850, in resolve\n",
      "        dist = best[req.key] = env.best_match(req, ws, installer)\n",
      "      File \"/mnt/yarn/usercache/livy/appcache/application_1687300297410_0002/container_1687300297410_0002_01_000001/tmp/1687303293099-0/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 1122, in best_match\n",
      "        return self.obtain(req, installer)\n",
      "      File \"/mnt/yarn/usercache/livy/appcache/application_1687300297410_0002/container_1687300297410_0002_01_000001/tmp/1687303293099-0/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 1134, in obtain\n",
      "        return installer(requirement)\n",
      "      File \"/mnt/yarn/usercache/livy/appcache/application_1687300297410_0002/container_1687300297410_0002_01_000001/tmp/1687303293099-0/lib/python3.7/site-packages/setuptools/dist.py\", line 429, in fetch_build_egg\n",
      "        return cmd.easy_install(req)\n",
      "      File \"/mnt/yarn/usercache/livy/appcache/application_1687300297410_0002/container_1687300297410_0002_01_000001/tmp/1687303293099-0/lib/python3.7/site-packages/setuptools/command/easy_install.py\", line 659, in easy_install\n",
      "        raise DistutilsError(msg)\n",
      "    distutils.errors.DistutilsError: Could not find suitable distribution for Requirement.parse('pypandoc')\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output."
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package('pandas==1.3.5')\n",
    "sc.install_pypi_package('boto3')\n",
    "sc.install_pypi_package('pyspark==2.3.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-20T23:39:02.429022Z",
     "iopub.status.busy": "2023-06-20T23:39:02.428653Z",
     "iopub.status.idle": "2023-06-20T23:39:11.727206Z",
     "shell.execute_reply": "2023-06-20T23:39:11.726314Z",
     "shell.execute_reply.started": "2023-06-20T23:39:02.428994Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4eed775a17843e5a7041508a10622c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "'DataFrame' object has no attribute 'query'\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1687300297410_0002/container_1687300297410_0002_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 1990, in __getattr__\n",
      "    \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name)\n",
      "AttributeError: 'DataFrame' object has no attribute 'query'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import re\n",
    "import tempfile\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import subprocess\n",
    "from pyspark.sql.functions import udf, col, lit\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\"\"\"\n",
    "Import Parquet As a DataFrame\n",
    "\"\"\"\n",
    "\n",
    "#Read in parquet file from public S3 bucket\n",
    "parquet_s3 = \"s3://macaquenaive/parquet/\"\n",
    "df_spark = spark.read.parquet(parquet_s3)\n",
    "\n",
    "##Verify count \n",
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T18:13:00.395023Z",
     "iopub.status.busy": "2023-04-12T18:13:00.394696Z",
     "iopub.status.idle": "2023-04-12T18:13:25.762002Z",
     "shell.execute_reply": "2023-04-12T18:13:25.761091Z",
     "shell.execute_reply.started": "2023-04-12T18:13:00.394992Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11239ca98404443c8d5ac506ca8aac88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 308 sequences"
     ]
    }
   ],
   "source": [
    "class Query():\n",
    "    \n",
    "    '''An example query class to hold query parameters'''\n",
    "    \n",
    "    def __init__(self,q_name,length='',v_call_top=\"\",d_call_top=\"\",j_call_top=\"\",regex=\"\",animal_id=\"\",v_mutation=''):\n",
    "        self.query_name = q_name\n",
    "        self.v_call_top = v_call_top\n",
    "        self.j_call_top = j_call_top\n",
    "        self.d_call_top = d_call_top\n",
    "        self.animal_id = animal_id\n",
    "        self.v_mutation = v_mutation\n",
    "        \n",
    "        if not length:\n",
    "            raise Exception(\"Length must be supplied\")\n",
    "        self.length = length\n",
    "        self.regular_expression = regex\n",
    "    \n",
    "    \n",
    "    \n",
    "    def apply(self,df):\n",
    "        \n",
    "        '''Apply function will take in spark dataframe and apply query parameters to it if they exist\n",
    "        \n",
    "           Returns a filtered dataframe\n",
    "        '''\n",
    "        self.queried_dataframe = \"\"\n",
    "        \n",
    "        ##Lets get length\n",
    "        ##self.queried_dataframe = df.filter(F.length(df.cdr3_aa) == self.length)\n",
    "        \n",
    "        self.queried_dataframe = df.filter(F.length(df.cdr3_aa) > self.length)\n",
    "        \n",
    "        ##If the rest of these were specified, add them to the filter\n",
    "       \n",
    "        if self.v_call_top:\n",
    "            self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.v_call_top == self.v_call_top)\n",
    "     \n",
    "        #if self.d_call_top:\n",
    "            #self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.d_call_top == self.d_call_top)     \n",
    "            \n",
    "        if self.d_call_top:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.d_call_top.rlike(self.d_call_top))\n",
    "                \n",
    "        if self.j_call_top:\n",
    "            self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.j_call_top == self.j_call_top)       \n",
    "        \n",
    "\n",
    "        if self.regular_expression:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.cdr3_aa.rlike(self.regular_expression))\n",
    "        \n",
    "        if self.animal_id:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.animal_id == self.animal_id)\n",
    "        \n",
    "        if self.v_mutation:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.v_mutation < self.v_mutation)\n",
    "            \n",
    "        print(\"Found {} sequences\".format(self.queried_dataframe.count()))\n",
    "        return self.queried_dataframe\n",
    "        \n",
    "    \n",
    "def write_out_hadoop_to_local(df, s3):\n",
    "    '''Because spark is distributed, writing out results is done by each worker node. As a result, it breaks up the results into partions. \n",
    "    \n",
    "    This function will search the hadoop file system and concatenate the parted file back to a single CSV file\n",
    "    \n",
    "    params df - the quried dataframe\n",
    "    s3 - the local file path (make sure its chmod 777)\n",
    "    '''\n",
    "    \n",
    "    #Output Name\n",
    "    output_s3 = s3\n",
    "    \n",
    "    #A temporary file to write out Hadoop CSV\n",
    "    t = tempfile.NamedTemporaryFile(delete=False)\n",
    "    \n",
    "\n",
    "    #Write out the CSV\n",
    "    df.write.csv(t.name,mode='overwrite')\n",
    "    with open(output_s3,'w') as f:\n",
    "        f.write(\",\".join(df.columns)+'\\n')\n",
    "        \n",
    "        print(\"Running HDFS concatenate\\n{} > {}\".format(' '.join(['hdfs','dfs','-cat','{}/*csv'.format(t.name)]),output_s3))\n",
    "        subprocess.Popen(['hdfs','dfs','-cat','{}/*csv'.format(t.name)],stdout=f)\n",
    "        \n",
    "\n",
    "    \n",
    "    print('Wrote CSV to {}'.format(output_s3))\n",
    "\n",
    "\n",
    "\n",
    "#query is IGHD3-3/J6 of length 23 with an FGV motif at the 7th index position of the HCDR3\n",
    "#my_query = Query('BG18_search.1',length=21)\n",
    "#my_query = Query('BG18_search.2',d_call_top=r'IGHD3-41',length=21,v_mutation=0.03)\n",
    "#my_query = Query('BG18_search.3',d_call_top=r'IGHD3-41',length=21,regex=r'TIFG')\n",
    "#my_query_4_7 = Query('BG18_search.4.7',d_call_top=r'IGHD3-41',length=21,regex=r'^....TIFG')\n",
    "#my_query_4_8 = Query('BG18_search.4.8',d_call_top=r'IGHD3-41',length=21,regex=r'^.....TIFG')\n",
    "#my_query_4_9 = Query('BG18_search.4.9',d_call_top=r'IGHD3-41',length=21,regex=r'^......TIFG')\n",
    "#my_query_5_7 = Query('BG18_search.5.7',d_call_top=r'IGHD3-41',length=21,regex=r'^....TIFG.....E')\n",
    "#my_query_5_8 = Query('BG18_search.5.8',d_call_top=r'IGHD3-41',length=21,regex=r'^.....TIFG.....E')\n",
    "#my_query_5_9 = Query('BG18_search.5.9',d_call_top=r'IGHD3-41',length=21,regex=r'^......TIFG.....E')\n",
    "my_query = Query('BG18_search.6',d_call_top=r'IGHD3-41',length=21, regex=r'TIFG.....E',v_mutation=0.03)\n",
    "#my_query = Query('BG18_search.7',d_call_top=r'IGHD3-41',length=1,v_mutation=0.03)\n",
    "\n",
    "#To run query, pass the input object from above to apply\n",
    "queried_df = my_query.apply(df_spark)\n",
    "#queried4_7_df = my_query_5_7.apply(df_spark)\n",
    "#queried4_8_df = my_query_5_8.apply(df_spark)\n",
    "#queried4_9_df = my_query_5_9.apply(df_spark)\n",
    "\n",
    "# #To get an output, we can write the matching antibodies as a CSV file to our local file system from the Hadoop File System \n",
    "# write_out_hadoop_to_local(queried_df,'/home/hadoop/output/mycsvoutput.csv')\n",
    "# pct64_df = determine_freq_of_junction(r'.YDFWS.............V$','IGHJ6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-20T23:24:08.732571Z",
     "iopub.status.busy": "2023-06-20T23:24:08.732231Z",
     "iopub.status.idle": "2023-06-20T23:24:44.131174Z",
     "shell.execute_reply": "2023-06-20T23:24:44.130333Z",
     "shell.execute_reply.started": "2023-06-20T23:24:08.732544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee5e613cd8b459d81352029eccd2820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 394 sequences"
     ]
    }
   ],
   "source": [
    "## This cell searches for IgM precursors and calculate the IgM precursor frequencies of all animals with c_call=\"IGHM*03\".\n",
    "## However, for dataset of animals RPz18, RGp18, RPb18, and REt18, only IgMs were sorted and sequenced, and some of their c_call reads were incomplete and therefore not reliable.\n",
    "## To address this issue, we used the next cell to analyze precursor frequencies of these four animals separately from this cell.\n",
    "\n",
    "class Query():\n",
    "    \n",
    "    '''An example query class to hold query parameters'''\n",
    "    \n",
    "    def __init__(self,q_name,length='',d_call_top=\"\",regex=\"\",animal_id=\"\",c_call=\"\",file_name=\"\"):\n",
    "        self.query_name = q_name\n",
    "        self.d_call_top = d_call_top\n",
    "        self.animal_id = animal_id\n",
    "        self.c_call = c_call\n",
    "        self.file_name = file_name\n",
    "        \n",
    "        if not length:\n",
    "            raise Exception(\"Length must be supplied\")\n",
    "        self.length = length\n",
    "        self.regular_expression = regex\n",
    "    \n",
    "    \n",
    "    \n",
    "    def apply(self,df):\n",
    "        \n",
    "        '''Apply function will take in spark dataframe and apply query parameters to it if they exist\n",
    "        \n",
    "           Returns a filtered dataframe\n",
    "        '''\n",
    "        self.queried_dataframe = \"\"\n",
    "        \n",
    "        ##Lets get length\n",
    "        \n",
    "        self.queried_dataframe = df.filter(F.length(df.cdr3_aa) > self.length)\n",
    "        \n",
    "        ##If the rest of these were specified, add them to the filter\n",
    "                   \n",
    "        if self.d_call_top:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.d_call_top.rlike(self.d_call_top))\n",
    "                \n",
    "        if self.c_call:\n",
    "            self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.c_call == self.c_call)      \n",
    "\n",
    "        if self.regular_expression:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.cdr3_aa.rlike(self.regular_expression))\n",
    "        \n",
    "        if self.animal_id:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.animal_id == self.animal_id)\n",
    "                \n",
    "        if self.file_name:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.file_name.rlike(self.file_name))\n",
    "                \n",
    "        print(\"Found {} sequences\".format(self.queried_dataframe.count()))\n",
    "        return self.queried_dataframe\n",
    "        \n",
    "    \n",
    "def write_out_hadoop_to_local(df, s3):\n",
    "    '''Because spark is distributed, writing out results is done by each worker node. As a result, it breaks up the results into partions. \n",
    "    \n",
    "    This function will search the hadoop file system and concatenate the parted file back to a single CSV file\n",
    "    \n",
    "    params df - the quried dataframe\n",
    "    s3 - the local file path (make sure its chmod 777)\n",
    "    '''\n",
    "    \n",
    "    #Output Name\n",
    "    output_s3 = s3\n",
    "    \n",
    "    #A temporary file to write out Hadoop CSV\n",
    "    t = tempfile.NamedTemporaryFile(delete=False)\n",
    "    \n",
    "\n",
    "    #Write out the CSV\n",
    "    df.write.csv(t.name,mode='overwrite')\n",
    "    with open(output_s3,'w') as f:\n",
    "        f.write(\",\".join(df.columns)+'\\n')\n",
    "        \n",
    "        print(\"Running HDFS concatenate\\n{} > {}\".format(' '.join(['hdfs','dfs','-cat','{}/*csv'.format(t.name)]),output_s3))\n",
    "        subprocess.Popen(['hdfs','dfs','-cat','{}/*csv'.format(t.name)],stdout=f)\n",
    "        \n",
    "\n",
    "    \n",
    "    print('Wrote CSV to {}'.format(output_s3))\n",
    "my_query_0 = Query('BG18_search.0',length=1,c_call=\"IGHM*03\")\n",
    "my_query_1 = Query('BG18_search.1',length=21,c_call=\"IGHM*03\")\n",
    "my_query_2 = Query('BG18_search.2',d_call_top=r'IGHD3-41',length=21,c_call=\"IGHM*03\")\n",
    "my_query_3 = Query('BG18_search.3',d_call_top=r'IGHD3-41',length=21,regex=r'IFG[VL]',c_call=\"IGHM*03\")\n",
    "my_query_4_7 = Query('BG18_search.4.7',d_call_top=r'IGHD3-41',length=21,regex=r'^.....IFG[VL]',c_call=\"IGHM*03\")\n",
    "my_query_4_8 = Query('BG18_search.4.8',d_call_top=r'IGHD3-41',length=21,regex=r'^......IFG[VL]',c_call=\"IGHM*03\")\n",
    "my_query_4_9 = Query('BG18_search.4.9',d_call_top=r'IGHD3-41',length=21,regex=r'^.......IFG[VL]',c_call=\"IGHM*03\")\n",
    "my_query_5_7 = Query('BG18_search.5.7',d_call_top=r'IGHD3-41',length=21,regex=r'^.....IFG[VL]....E',c_call=\"IGHM*03\")\n",
    "my_query_5_8 = Query('BG18_search.5.8',d_call_top=r'IGHD3-41',length=21,regex=r'^......IFG[VL]....E',c_call=\"IGHM*03\")\n",
    "my_query_5_9 = Query('BG18_search.5.9',d_call_top=r'IGHD3-41',length=21,regex=r'^.......IFG[VL]....E',c_call=\"IGHM*03\")\n",
    "my_query_6 = Query('BG18_search.6',d_call_top=r'IGHD3-41',length=21, regex=r'IFG[VL]....E',c_call=\"IGHM*03\")\n",
    "my_query_7 = Query('BG18_search.7',d_call_top=r'IGHD3-41',length=1,c_call=\"IGHM*03\")\n",
    "\n",
    "#To run query, pass the input object from above to apply\n",
    "\n",
    "queried0_df = my_query_0.apply(df_spark)\n",
    "queried1_df = my_query_1.apply(df_spark)\n",
    "queried2_df = my_query_2.apply(df_spark)\n",
    "queried3_df = my_query_3.apply(df_spark)\n",
    "queried4_7_df = my_query_4_7.apply(df_spark)\n",
    "queried4_8_df = my_query_4_8.apply(df_spark)\n",
    "queried4_9_df = my_query_4_9.apply(df_spark)\n",
    "queried5_7_df = my_query_5_7.apply(df_spark)\n",
    "queried5_8_df = my_query_5_8.apply(df_spark)\n",
    "queried5_9_df = my_query_5_9.apply(df_spark)\n",
    "queried6_df = my_query_6.apply(df_spark)\n",
    "queried7_df = my_query_7.apply(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-20T23:24:08.732571Z",
     "iopub.status.busy": "2023-06-20T23:24:08.732231Z",
     "iopub.status.idle": "2023-06-20T23:24:44.131174Z",
     "shell.execute_reply": "2023-06-20T23:24:44.130333Z",
     "shell.execute_reply.started": "2023-06-20T23:24:08.732544Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee5e613cd8b459d81352029eccd2820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 394 sequences"
     ]
    }
   ],
   "source": [
    "#This cell searches for precursors and calculate the precursor frequencies of RPz18, RGp18, RPb18, and REt18 only.\n",
    "## The dataset of animals RPz18, RGp18, RPb18, and REt18, only IgMs were sorted and sequenced, and some of their c_call reads were incomplete and therefore not reliable.\n",
    "## To address this issue, we used this cell to analyze precursor frequencies of these four animals separately from the previous cell.\n",
    "\n",
    "class Query():\n",
    "    \n",
    "    '''An example query class to hold query parameters'''\n",
    "    \n",
    "    def __init__(self,q_name,length='',d_call_top=\"\",regex=\"\",animal_id=\"\",c_call=\"\",file_name=\"\"):\n",
    "        self.query_name = q_name\n",
    "        self.d_call_top = d_call_top\n",
    "        self.animal_id = animal_id\n",
    "        self.c_call = c_call\n",
    "        self.file_name = file_name\n",
    "        \n",
    "        if not length:\n",
    "            raise Exception(\"Length must be supplied\")\n",
    "        self.length = length\n",
    "        self.regular_expression = regex\n",
    "    \n",
    "    \n",
    "    \n",
    "    def apply(self,df):\n",
    "        \n",
    "        '''Apply function will take in spark dataframe and apply query parameters to it if they exist\n",
    "        \n",
    "           Returns a filtered dataframe\n",
    "        '''\n",
    "        self.queried_dataframe = \"\"\n",
    "        \n",
    "        ##Lets get length\n",
    "        \n",
    "        self.queried_dataframe = df.filter(F.length(df.cdr3_aa) > self.length)\n",
    "        \n",
    "        ##If the rest of these were specified, add them to the filter\n",
    "                   \n",
    "        if self.d_call_top:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.d_call_top.rlike(self.d_call_top))\n",
    "                \n",
    "        if self.c_call:\n",
    "            self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.c_call == self.c_call)      \n",
    "\n",
    "        if self.regular_expression:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.cdr3_aa.rlike(self.regular_expression))\n",
    "        \n",
    "        if self.animal_id:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.animal_id == self.animal_id)\n",
    "                \n",
    "        if self.file_name:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.file_name.rlike(self.file_name))\n",
    "                \n",
    "        print(\"Found {} sequences\".format(self.queried_dataframe.count()))\n",
    "        return self.queried_dataframe\n",
    "        \n",
    "    \n",
    "def write_out_hadoop_to_local(df, s3):\n",
    "    '''Because spark is distributed, writing out results is done by each worker node. As a result, it breaks up the results into partions. \n",
    "    \n",
    "    This function will search the hadoop file system and concatenate the parted file back to a single CSV file\n",
    "    \n",
    "    params df - the quried dataframe\n",
    "    s3 - the local file path (make sure its chmod 777)\n",
    "    '''\n",
    "    \n",
    "    #Output Name\n",
    "    output_s3 = s3\n",
    "    \n",
    "    #A temporary file to write out Hadoop CSV\n",
    "    t = tempfile.NamedTemporaryFile(delete=False)\n",
    "    \n",
    "\n",
    "    #Write out the CSV\n",
    "    df.write.csv(t.name,mode='overwrite')\n",
    "    with open(output_s3,'w') as f:\n",
    "        f.write(\",\".join(df.columns)+'\\n')\n",
    "        \n",
    "        print(\"Running HDFS concatenate\\n{} > {}\".format(' '.join(['hdfs','dfs','-cat','{}/*csv'.format(t.name)]),output_s3))\n",
    "        subprocess.Popen(['hdfs','dfs','-cat','{}/*csv'.format(t.name)],stdout=f)\n",
    "        \n",
    "\n",
    "    \n",
    "    print('Wrote CSV to {}'.format(output_s3))\n",
    "my_query_0 = Query('BG18_search.0',length=1,file_name=r'_Functional')\n",
    "my_query_1 = Query('BG18_search.1',length=21,file_name=r'_Functional')\n",
    "my_query_2 = Query('BG18_search.2',d_call_top=r'IGHD3-41',length=21,file_name=r'_Functional')\n",
    "my_query_3 = Query('BG18_search.3',d_call_top=r'IGHD3-41',length=21,regex=r'IFG[VL]',file_name=r'_Functional')\n",
    "my_query_4_7 = Query('BG18_search.4.7',d_call_top=r'IGHD3-41',length=21,regex=r'^.....IFG[VL]',file_name=r'_Functional')\n",
    "my_query_4_8 = Query('BG18_search.4.8',d_call_top=r'IGHD3-41',length=21,regex=r'^......IFG[VL]',file_name=r'_Functional')\n",
    "my_query_4_9 = Query('BG18_search.4.9',d_call_top=r'IGHD3-41',length=21,regex=r'^.......IFG[VL]',file_name=r'_Functional')\n",
    "my_query_5_7 = Query('BG18_search.5.7',d_call_top=r'IGHD3-41',length=21,regex=r'^.....IFG[VL]....E',file_name=r'_Functional')\n",
    "my_query_5_8 = Query('BG18_search.5.8',d_call_top=r'IGHD3-41',length=21,regex=r'^......IFG[VL]....E',file_name=r'_Functional')\n",
    "my_query_5_9 = Query('BG18_search.5.9',d_call_top=r'IGHD3-41',length=21,regex=r'^.......IFG[VL]....E',file_name=r'_Functional')\n",
    "my_query_6 = Query('BG18_search.6',d_call_top=r'IGHD3-41',length=21, regex=r'IFG[VL]....E',file_name=r'_Functional')\n",
    "my_query_7 = Query('BG18_search.7',d_call_top=r'IGHD3-41',length=1,file_name=r'_Functional')\n",
    "\n",
    "#To run query, pass the input object from above to apply\n",
    "\n",
    "queried0_df = my_query_0.apply(df_spark)\n",
    "queried1_df = my_query_1.apply(df_spark)\n",
    "queried2_df = my_query_2.apply(df_spark)\n",
    "queried3_df = my_query_3.apply(df_spark)\n",
    "queried4_7_df = my_query_4_7.apply(df_spark)\n",
    "queried4_8_df = my_query_4_8.apply(df_spark)\n",
    "queried4_9_df = my_query_4_9.apply(df_spark)\n",
    "queried5_7_df = my_query_5_7.apply(df_spark)\n",
    "queried5_8_df = my_query_5_8.apply(df_spark)\n",
    "queried5_9_df = my_query_5_9.apply(df_spark)\n",
    "queried6_df = my_query_6.apply(df_spark)\n",
    "queried7_df = my_query_7.apply(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T04:38:11.605360Z",
     "iopub.status.busy": "2023-05-31T04:38:11.605040Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291f6218ed7640f3a52a306b58a18672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a41b23be0247668609d97e91b314f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pandas0_df = queried0_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas1_df = queried1_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas2_df = queried2_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas3_df = queried3_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas47_df = queried4_7_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas48_df = queried4_8_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas49_df = queried4_9_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas57_df = queried5_7_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas58_df = queried5_8_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas59_df = queried5_9_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas6_df = queried6_df.select('sequence_id','animal_id').toPandas()\n",
    "pandas7_df = queried7_df.select('sequence_id','animal_id').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas4_df = pandas.concat([pandas47_df,pandas48_df,pandas49_df])\n",
    "pandas5_df = pandas.concat([pandas57_df,pandas58_df,pandas59_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-06-20T23:42:28.680257Z",
     "iopub.status.busy": "2023-06-20T23:42:28.679936Z",
     "iopub.status.idle": "2023-06-20T23:42:28.734638Z",
     "shell.execute_reply": "2023-06-20T23:42:28.732857Z",
     "shell.execute_reply.started": "2023-06-20T23:42:28.680230Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382b91c9be8a4b50beaebd2a6ae9311e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           count\n",
      "animal_id       \n",
      "zRh15          1\n",
      "zRh03          1\n",
      "A3             1\n",
      "A4             1\n",
      "Rh6            1\n",
      "Rh2            1\n",
      "Rh-A13003      1\n",
      "F130           1\n",
      "E09            1\n",
      "D11            2\n",
      "D19            2\n",
      "Rh1            2\n",
      "A1             3\n",
      "A5             3\n",
      "zRh14          4\n",
      "E10            4\n",
      "D15            7\n",
      "zRh06          7\n",
      "A2             8\n",
      "E07           11\n",
      "E08           11\n",
      "E06           17\n",
      "E04           19\n",
      "zRh11         20\n",
      "D20           21\n",
      "D7            24\n",
      "E05           45\n",
      "E03           68\n",
      "E01          107"
     ]
    }
   ],
   "source": [
    "#print out the number of IgM sequences from all animals  \n",
    "counts = pandas0_df.groupby('animal_id').count().rename({'sequence_id':'count'},axis=1).sort_values('count')\n",
    "pandas.set_option('display.max_rows', None)  \n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the number of IgM sequences from all animals in search 1  \n",
    "counts = pandas1_df.groupby('animal_id').count().rename({'sequence_id':'count'},axis=1).sort_values('count')\n",
    "pandas.set_option('display.max_rows', None)  \n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the number of IgM sequences from all animals in search 2  \n",
    "counts = pandas2_df.groupby('animal_id').count().rename({'sequence_id':'count'},axis=1).sort_values('count')\n",
    "pandas.set_option('display.max_rows', None)  \n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the number of IgM sequences from all animals in search 3  \n",
    "counts = pandas3_df.groupby('animal_id').count().rename({'sequence_id':'count'},axis=1).sort_values('count')\n",
    "pandas.set_option('display.max_rows', None)  \n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the number of IgM sequences from all animals in search 4 \n",
    "counts = pandas4_df.groupby('animal_id').count().rename({'sequence_id':'count'},axis=1).sort_values('count')\n",
    "pandas.set_option('display.max_rows', None)  \n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the number of IgM sequences from all animals in search 5  \n",
    "counts = pandas5_df.groupby('animal_id').count().rename({'sequence_id':'count'},axis=1).sort_values('count')\n",
    "pandas.set_option('display.max_rows', None)  \n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-06-20T23:43:48.706371Z",
     "iopub.status.busy": "2023-06-20T23:43:48.706057Z",
     "iopub.status.idle": "2023-06-20T23:43:48.766656Z",
     "shell.execute_reply": "2023-06-20T23:43:48.765936Z",
     "shell.execute_reply.started": "2023-06-20T23:43:48.706344Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540e330eddde4e509bb877f5d29b27ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           count\n",
      "animal_id       \n",
      "zRh15          1\n",
      "zRh03          1\n",
      "A3             1\n",
      "A4             1\n",
      "Rh6            1\n",
      "Rh2            1\n",
      "Rh-A13003      1\n",
      "F130           1\n",
      "E09            1\n",
      "D11            2\n",
      "D19            2\n",
      "Rh1            2\n",
      "A1             3\n",
      "A5             3\n",
      "zRh14          4\n",
      "E10            4\n",
      "D15            7\n",
      "zRh06          7\n",
      "A2             8\n",
      "E07           11\n",
      "E08           11\n",
      "E06           17\n",
      "E04           19\n",
      "zRh11         20\n",
      "D20           21\n",
      "D7            24\n",
      "E05           45\n",
      "E03           68\n",
      "E01          107"
     ]
    }
   ],
   "source": [
    "#print out the number of IgM sequences from all animals in search 6  \n",
    "counts = pandas6_df.groupby('animal_id').count().rename({'sequence_id':'count'},axis=1).sort_values('count')\n",
    "pandas.set_option('display.max_rows', None)  \n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-06-20T23:43:48.706371Z",
     "iopub.status.busy": "2023-06-20T23:43:48.706057Z",
     "iopub.status.idle": "2023-06-20T23:43:48.766656Z",
     "shell.execute_reply": "2023-06-20T23:43:48.765936Z",
     "shell.execute_reply.started": "2023-06-20T23:43:48.706344Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540e330eddde4e509bb877f5d29b27ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           count\n",
      "animal_id       \n",
      "zRh15          1\n",
      "zRh03          1\n",
      "A3             1\n",
      "A4             1\n",
      "Rh6            1\n",
      "Rh2            1\n",
      "Rh-A13003      1\n",
      "F130           1\n",
      "E09            1\n",
      "D11            2\n",
      "D19            2\n",
      "Rh1            2\n",
      "A1             3\n",
      "A5             3\n",
      "zRh14          4\n",
      "E10            4\n",
      "D15            7\n",
      "zRh06          7\n",
      "A2             8\n",
      "E07           11\n",
      "E08           11\n",
      "E06           17\n",
      "E04           19\n",
      "zRh11         20\n",
      "D20           21\n",
      "D7            24\n",
      "E05           45\n",
      "E03           68\n",
      "E01          107"
     ]
    }
   ],
   "source": [
    "#print out the number of IgM sequences from all animals in search 7  \n",
    "counts = pandas7_df.groupby('animal_id').count().rename({'sequence_id':'count'},axis=1).sort_values('count')\n",
    "pandas.set_option('display.max_rows', None)  \n",
    "print(counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
