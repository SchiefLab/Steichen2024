{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-06-20T23:06:33.475264Z",
     "iopub.status.busy": "2023-06-20T23:06:33.474927Z",
     "iopub.status.idle": "2023-06-20T23:07:47.384757Z",
     "shell.execute_reply": "2023-06-20T23:07:47.383044Z",
     "shell.execute_reply.started": "2023-06-20T23:06:33.475235Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e1936bdfcf47f69afe1f0fec4836bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1687300297410_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-37-126.us-west-2.compute.internal:20888/proxy/application_1687300297410_0001/\" class=\"emr-proxy-link j-2J5FQLC0SEOTU application_1687300297410_0001\" emr-resource=\"j-2J5FQLC0SEOTU\n\" application-id=\"application_1687300297410_0001\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-43-73.us-west-2.compute.internal:8042/node/containerlogs/container_1687300297410_0001_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.3.5\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /usr/local/lib64/python3.7/site-packages (from pandas==1.3.5) (1.20.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/site-packages (from pandas==1.3.5) (2022.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.3.5) (1.13.0)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-1.3.5 python-dateutil-2.8.2\n",
      "\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.26.157-py3-none-any.whl (135 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3) (1.0.1)\n",
      "Collecting botocore<1.30.0,>=1.29.157\n",
      "  Downloading botocore-1.29.157-py3-none-any.whl (10.9 MB)\n",
      "Collecting urllib3<1.27,>=1.25.4\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./tmp/1687302430604-0/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.157->boto3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.157->boto3) (1.13.0)\n",
      "Installing collected packages: urllib3, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.26.157 botocore-1.29.157 s3transfer-0.6.1 urllib3-1.26.16\n",
      "\n",
      "Collecting pyspark==2.3.4\n",
      "  Downloading pyspark-2.3.4.tar.gz (212.3 MB)\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: /mnt/yarn/usercache/livy/appcache/application_1687300297410_0001/container_1687300297410_0001_01_000001/tmp/1687302430604-0/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/mnt/tmp/pip-install-l3jhof9e/pyspark/setup.py'\"'\"'; __file__='\"'\"'/mnt/tmp/pip-install-l3jhof9e/pyspark/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /mnt/tmp/pip-pip-egg-info-8ddadlrp\n",
      "         cwd: /mnt/tmp/pip-install-l3jhof9e/pyspark/\n",
      "    Complete output (24 lines):\n",
      "    Could not import pypandoc - required to package PySpark\n",
      "    Couldn't find index page for 'pypandoc' (maybe misspelled?)\n",
      "    No local packages or working download links found for pypandoc\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/mnt/tmp/pip-install-l3jhof9e/pyspark/setup.py\", line 224, in <module>\n",
      "        'Programming Language :: Python :: Implementation :: PyPy']\n",
      "      File \"/usr/lib64/python3.7/distutils/core.py\", line 108, in setup\n",
      "        _setup_distribution = dist = klass(attrs)\n",
      "      File \"/mnt/yarn/usercache/livy/appcache/application_1687300297410_0001/container_1687300297410_0001_01_000001/tmp/1687302430604-0/lib/python3.7/site-packages/setuptools/dist.py\", line 315, in __init__\n",
      "        self.fetch_build_eggs(attrs['setup_requires'])\n",
      "      File \"/mnt/yarn/usercache/livy/appcache/application_1687300297410_0001/container_1687300297410_0001_01_000001/tmp/1687302430604-0/lib/python3.7/site-packages/setuptools/dist.py\", line 361, in fetch_build_eggs\n",
      "        replace_conflicting=True,\n",
      "      File \"/mnt/yarn/usercache/livy/appcache/application_1687300297410_0001/container_1687300297410_0001_01_000001/tmp/1687302430604-0/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 850, in resolve\n",
      "        dist = best[req.key] = env.best_match(req, ws, installer)\n",
      "      File \"/mnt/yarn/usercache/livy/appcache/application_1687300297410_0001/container_1687300297410_0001_01_000001/tmp/1687302430604-0/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 1122, in best_match\n",
      "        return self.obtain(req, installer)\n",
      "      File \"/mnt/yarn/usercache/livy/appcache/application_1687300297410_0001/container_1687300297410_0001_01_000001/tmp/1687302430604-0/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 1134, in obtain\n",
      "        return installer(requirement)\n",
      "      File \"/mnt/yarn/usercache/livy/appcache/application_1687300297410_0001/container_1687300297410_0001_01_000001/tmp/1687302430604-0/lib/python3.7/site-packages/setuptools/dist.py\", line 429, in fetch_build_egg\n",
      "        return cmd.easy_install(req)\n",
      "      File \"/mnt/yarn/usercache/livy/appcache/application_1687300297410_0001/container_1687300297410_0001_01_000001/tmp/1687302430604-0/lib/python3.7/site-packages/setuptools/command/easy_install.py\", line 659, in easy_install\n",
      "        raise DistutilsError(msg)\n",
      "    distutils.errors.DistutilsError: Could not find suitable distribution for Requirement.parse('pypandoc')\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output."
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package('pandas==1.3.5')\n",
    "sc.install_pypi_package('boto3')\n",
    "sc.install_pypi_package('pyspark==2.3.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-20T23:08:08.178996Z",
     "iopub.status.busy": "2023-06-20T23:08:08.178684Z",
     "iopub.status.idle": "2023-06-20T23:09:15.768849Z",
     "shell.execute_reply": "2023-06-20T23:09:15.768121Z",
     "shell.execute_reply.started": "2023-06-20T23:08:08.178971Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0685252ab47a47779e2985be0e7d24aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1112960406"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import re\n",
    "import tempfile\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import subprocess\n",
    "from pyspark.sql.functions import udf, col, lit\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\"\"\"\n",
    "Import Parquet As a DataFrame\n",
    "\"\"\"\n",
    "\n",
    "##Read in parquet file from public S3 bucket\n",
    "parquet_s3 = \"s3://steichenetalpublicdata/analyzed_sequences/parquet\"\n",
    "df_spark = spark.read.parquet(parquet_s3)\n",
    "\n",
    "##Verify count \n",
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-20T23:09:45.743489Z",
     "iopub.status.busy": "2023-06-20T23:09:45.743160Z",
     "iopub.status.idle": "2023-06-20T23:12:43.771692Z",
     "shell.execute_reply": "2023-06-20T23:12:43.770801Z",
     "shell.execute_reply.started": "2023-06-20T23:09:45.743455Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eefc1f1501f54d1dae8753bfc243d19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57075 sequences"
     ]
    }
   ],
   "source": [
    "class Query():\n",
    "    \n",
    "    '''An example query class to hold query parameters'''\n",
    "    \n",
    "    def __init__(self,q_name,length='',v_fam=\"\",v_gene=\"\",d_gene=\"\",j_gene=\"\",regex=\"\",ez_donor=\"\"):\n",
    "        self.query_name = q_name\n",
    "        self.v_fam = v_fam\n",
    "        self.v_gene = v_gene\n",
    "        self.j_gene = j_gene\n",
    "        self.d_gene = d_gene\n",
    "        self.ez_donor = ez_donor\n",
    "        \n",
    "        if not length:\n",
    "            raise Exception(\"Length must be supplied\")\n",
    "        self.length = length\n",
    "        self.regular_expression = regex\n",
    "    \n",
    "    \n",
    "    \n",
    "    def apply(self,df):\n",
    "        \n",
    "        '''Apply function will take in spark dataframe and apply query parameters to it if they exist\n",
    "        \n",
    "           Returns a filtered dataframe\n",
    "        '''\n",
    "        self.queried_dataframe = \"\"\n",
    "        \n",
    "        ##Lets get length\n",
    "        \n",
    "        self.queried_dataframe = df.filter(F.length(df.cdr3_aa) > self.length)\n",
    "        \n",
    "        ##If the rest of these were specified, add them to the filter\n",
    "        if self.v_fam:\n",
    "            self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.v_fam == self.v_fam)\n",
    "        \n",
    "        if self.v_gene:\n",
    "            self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.v_gene == self.v_gene)\n",
    "     \n",
    "        if self.d_gene:\n",
    "            self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.d_gene == self.d_gene)       \n",
    "        \n",
    "        if self.j_gene:\n",
    "            self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.j_gene == self.j_gene)       \n",
    "        \n",
    "\n",
    "        if self.regular_expression:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.cdr3_aa.rlike(self.regular_expression))\n",
    "        \n",
    "        if self.ez_donor:\n",
    "             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.ez_donor == self.ez_donor)\n",
    "            \n",
    "        print(\"Found {} sequences\".format(self.queried_dataframe.count()))\n",
    "        return self.queried_dataframe\n",
    "        \n",
    "    \n",
    "def write_out_hadoop_to_local(df, s3):\n",
    "    '''Because spark is distributed, writing out results is done by each worker node. As a result, it breaks up the results into partions. \n",
    "    \n",
    "    This function will search the hadoop file system and concatenate the parted file back to a single CSV file\n",
    "    \n",
    "    params df - the quried dataframe\n",
    "    s3 - the local file path (make sure its chmod 777)\n",
    "    '''\n",
    "    \n",
    "    #Output Name\n",
    "    output_s3 = s3\n",
    "    \n",
    "    #A temporary file to write out Hadoop CSV\n",
    "    t = tempfile.NamedTemporaryFile(delete=False)\n",
    "    \n",
    "\n",
    "    #Write out the CSV\n",
    "    df.write.csv(t.name,mode='overwrite')\n",
    "    with open(output_s3,'w') as f:\n",
    "        f.write(\",\".join(df.columns)+'\\n')\n",
    "        \n",
    "        print(\"Running HDFS concatenate\\n{} > {}\".format(' '.join(['hdfs','dfs','-cat','{}/*csv'.format(t.name)]),output_s3))\n",
    "        subprocess.Popen(['hdfs','dfs','-cat','{}/*csv'.format(t.name)],stdout=f)\n",
    "        \n",
    "\n",
    "    \n",
    "    print('Wrote CSV to {}'.format(output_s3))\n",
    "\n",
    "my_query_1 = Query('BG18_search.1',length=21)\n",
    "my_query_2 = Query('BG18_search.2',d_gene='IGHD3-3',length=21)\n",
    "my_query_3 = Query('BG18_search.3',d_gene='IGHD3-3',length=21, regex=r'FGV')\n",
    "my_query_4_7 = Query('BG18_search.4.7',d_gene='IGHD3-3',length=21, regex=r'^......FGV')\n",
    "my_query_4_8 = Query('BG18_search.4.8',d_gene='IGHD3-3',length=21, regex=r'^.......FGV')\n",
    "my_query_4_9 = Query('BG18_search.4.9',d_gene='IGHD3-3',length=21, regex=r'^........FGV')\n",
    "my_query_5_7 = Query('BG18_search.5.7',d_gene='IGHD3-3',length=21, regex=r'^......FGV....E')\n",
    "my_query_5_8 = Query('BG18_search.5.8',d_gene='IGHD3-3',length=21, regex=r'^.......FGV....E')\n",
    "my_query_5_9 = Query('BG18_search.5.9',d_gene='IGHD3-3',length=21, regex=r'^........FGV....E')\n",
    "my_query_6 = Query('BG18_search.6',d_gene='IGHD3-3',length=21, regex=r'FGV....E')\n",
    "my_query_7 = Query('BG18_search.7',d_gene='IGHD3-3',length=1)\n",
    "\n",
    "#To run query, pass the input object from above to apply\n",
    "\n",
    "queried1_df = my_query_1.apply(df_spark)\n",
    "queried2_df = my_query_2.apply(df_spark)\n",
    "queried3_df = my_query_3.apply(df_spark)\n",
    "queried4_7_df = my_query_4_7.apply(df_spark)\n",
    "queried4_8_df = my_query_4_8.apply(df_spark)\n",
    "queried4_9_df = my_query_4_9.apply(df_spark)\n",
    "queried5_7_df = my_query_5_7.apply(df_spark)\n",
    "queried5_8_df = my_query_5_8.apply(df_spark)\n",
    "queried5_9_df = my_query_5_9.apply(df_spark)\n",
    "queried6_df = my_query_6.apply(df_spark)\n",
    "queried7_df = my_query_7.apply(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5164edaabdff4c079917e81902894c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pandas1_df = queried1_df.select('_id','ez_donor').toPandas()\n",
    "pandas2_df = queried2_df.select('_id','ez_donor').toPandas()\n",
    "pandas3_df = queried3_df.select('_id','ez_donor').toPandas()\n",
    "pandas47_df = queried4_7_df.select('_id','ez_donor').toPandas()\n",
    "pandas48_df = queried4_8_df.select('_id','ez_donor').toPandas()\n",
    "pandas49_df = queried4_9_df.select('_id','ez_donor').toPandas()\n",
    "pandas57_df = queried5_7_df.select('_id','ez_donor').toPandas()\n",
    "pandas58_df = queried5_8_df.select('_id','ez_donor').toPandas()\n",
    "pandas59_df = queried5_9_df.select('_id','ez_donor').toPandas()\n",
    "pandas6_df = queried6_df.select('_id','ez_donor').toPandas()\n",
    "pandas7_df = queried7_df.select('_id','ez_donor').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fecd59785a4f1db108ffe73ab10003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pandas4_df = pandas.concat([pandas47_df,pandas48_df,pandas49_df])\n",
    "pandas5_df = pandas.concat([pandas57_df,pandas58_df,pandas59_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3294f87eccb54961a4b354b397ec42e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = pandas1_df.groupby('ez_donor').count().rename({'_id':'count'},axis=1).sort_values('count')\n",
    "df_count = df_spark.groupby('ez_donor').count().toPandas()\n",
    "merged = counts.reset_index().merge(df_count, on='ez_donor')\n",
    "merged['normal'] = merged['count_x']/merged['count_y']*1000000\n",
    "merged.sort_values('ez_donor')[['ez_donor','normal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pandas2_df.groupby('ez_donor').count().rename({'_id':'count'},axis=1).sort_values('count')\n",
    "df_count = df_spark.groupby('ez_donor').count().toPandas()\n",
    "merged = counts.reset_index().merge(df_count, on='ez_donor')\n",
    "merged['normal'] = merged['count_x']/merged['count_y']*1000000\n",
    "merged.sort_values('ez_donor')[['ez_donor','normal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pandas3_df.groupby('ez_donor').count().rename({'_id':'count'},axis=1).sort_values('count')\n",
    "df_count = df_spark.groupby('ez_donor').count().toPandas()\n",
    "merged = counts.reset_index().merge(df_count, on='ez_donor')\n",
    "merged['normal'] = merged['count_x']/merged['count_y']*1000000\n",
    "merged.sort_values('ez_donor')[['ez_donor','normal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pandas4_df.groupby('ez_donor').count().rename({'_id':'count'},axis=1).sort_values('count')\n",
    "df_count = df_spark.groupby('ez_donor').count().toPandas()\n",
    "merged = counts.reset_index().merge(df_count, on='ez_donor')\n",
    "merged['normal'] = merged['count_x']/merged['count_y']*1000000\n",
    "merged.sort_values('ez_donor')[['ez_donor','normal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pandas5_df.groupby('ez_donor').count().rename({'_id':'count'},axis=1).sort_values('count')\n",
    "df_count = df_spark.groupby('ez_donor').count().toPandas()\n",
    "merged = counts.reset_index().merge(df_count, on='ez_donor')\n",
    "merged['normal'] = merged['count_x']/merged['count_y']*1000000\n",
    "merged.sort_values('ez_donor')[['ez_donor','normal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-06-20T23:18:44.470222Z",
     "iopub.status.busy": "2023-06-20T23:18:44.469911Z",
     "iopub.status.idle": "2023-06-20T23:19:31.922910Z",
     "shell.execute_reply": "2023-06-20T23:19:31.922103Z",
     "shell.execute_reply.started": "2023-06-20T23:18:44.470197Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa92f27aebf45699f81f0d9d112651f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ez_donor     normal\n",
      "9         1  22.440817\n",
      "8        10  52.467923\n",
      "1        11   4.229708\n",
      "6        12  25.441762\n",
      "10       13  26.772067\n",
      "7        14  14.038361\n",
      "11        2  75.570549\n",
      "13        3  65.397028\n",
      "12        4  57.016984\n",
      "3         5  18.864662\n",
      "2         6  13.998922\n",
      "0         7  11.643994\n",
      "4         8  30.660714\n",
      "5         9  30.799921"
     ]
    }
   ],
   "source": [
    "counts = pandas6_df.groupby('ez_donor').count().rename({'_id':'count'},axis=1).sort_values('count')\n",
    "df_count = df_spark.groupby('ez_donor').count().toPandas()\n",
    "merged = counts.reset_index().merge(df_count, on='ez_donor')\n",
    "merged['normal'] = merged['count_x']/merged['count_y']*1000000\n",
    "merged.sort_values('ez_donor')[['ez_donor','normal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pandas7_df.groupby('ez_donor').count().rename({'_id':'count'},axis=1).sort_values('count')\n",
    "df_count = df_spark.groupby('ez_donor').count().toPandas()\n",
    "merged = counts.reset_index().merge(df_count, on='ez_donor')\n",
    "merged['normal'] = merged['count_x']/merged['count_y']*1000000\n",
    "merged.sort_values('ez_donor')[['ez_donor','normal']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
