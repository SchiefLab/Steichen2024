{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "16617ca0-8003-424c-afc3-9614800c9657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sadie.airr.airrtable import LinkedAirrTable\n",
    "from sadie.reference import Reference\n",
    "from sadie.reference.yaml import YamlRef\n",
    "from Levenshtein import distance\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f9d4cd-0339-492e-bb99-8b18164c6ce5",
   "metadata": {},
   "source": [
    "# Read in and pre-process data\n",
    "\n",
    "This is all 10X VDJ contigs and sorted cells in one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d52836f8-f112-408b-b344-0df545e0c8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in combined dataframe\n",
    "all_combined = pd.read_feather(\"all_processed_combined.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2c29ffda-7e82-4aed-a70f-0253709bcbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to to a LinkedAirrTable (a heavy-light paired airrtable)\n",
    "lat = LinkedAirrTable(all_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f097e1f-b7c6-4267-bacc-ef2e3e938751",
   "metadata": {},
   "source": [
    "## Add Closest Orthologs and CDR3 Length\n",
    "\n",
    "here we are taking the macaque gene and finding the closest human ortholog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f69b7109-cd2d-4d79-b905-40b275564e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lookups(human_lookup, mac_lookup):\n",
    "    '''A simple function to find the best scoring ortholog between human and macaque'''\n",
    "    lookups = {}\n",
    "    for x in mac_lookup:\n",
    "        lowest_score = 1000\n",
    "        for y in human_lookup:\n",
    "            d = distance(human_lookup[y], mac_lookup[x])\n",
    "            if d < lowest_score:\n",
    "                lookups[x] = y\n",
    "                lowest_score = d\n",
    "    return lookups\n",
    "\n",
    "# enter no file to use reference.yml\n",
    "yml_ref = YamlRef()\n",
    "\n",
    "# create empty reference object\n",
    "ref_class = Reference()\n",
    "genes = yml_ref[\"human\"][\"imgt\"][\"human\"]\n",
    "ref_class.add_genes(**{\"genes\": genes, \"species\": \"human\", \"source\": \"imgt\"})\n",
    "\n",
    "# make human\n",
    "human_df = ref_class.get_dataframe()\n",
    "v_genes_human = human_df.query(\"gene_segment=='V'\")\n",
    "d_genes_human = human_df.query(\"gene_segment=='D'\")\n",
    "j_genes_human = human_df.query(\"gene_segment=='J'\")\n",
    "\n",
    "\n",
    "# create empty reference object\n",
    "ref_class = Reference()\n",
    "genes = yml_ref[\"macaque\"][\"custom\"][\"macaque\"]\n",
    "ref_class.add_genes(**{\"genes\": genes, \"species\": \"macaque\", \"source\": \"custom\"})\n",
    "\n",
    "# make macaque ref\n",
    "macaque_df = ref_class.get_dataframe()\n",
    "v_genes_macaque = macaque_df.query(\"gene_segment=='V'\")\n",
    "d_genes_macaque = macaque_df.query(\"gene_segment=='D'\")\n",
    "j_genes_macaque = macaque_df.query(\"gene_segment=='J'\")\n",
    "\n",
    "# now make a lookup table for macaque and human genes. \n",
    "mac_lookup_v = (\n",
    "    v_genes_macaque.query(\"gene_segment=='V'\")\n",
    "    .query(\"`imgt.contrived_functional`=='F'\")[[\"gene\", \"imgt.sequence_gapped_aa\"]]\n",
    "    .rename({\"imgt.sequence_gapped_aa\": \"sequence_aa\"}, axis=1)\n",
    "    .set_index(\"gene\")[\"sequence_aa\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "human_lookup_v = (\n",
    "    v_genes_human.query(\"gene_segment=='V'\")\n",
    "    .query(\"`imgt.contrived_functional`=='F'\")[[\"gene\", \"imgt.sequence_gapped_aa\"]]\n",
    "    .rename({\"imgt.sequence_gapped_aa\": \"sequence_aa\"}, axis=1)\n",
    "    .set_index(\"gene\")[\"sequence_aa\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# do the same for the J gene\n",
    "mac_lookup_j = (\n",
    "    j_genes_macaque.query(\"gene_segment=='J'\")[[\"gene\", \"imgt.sequence_gapped_aa\"]]\n",
    "    .rename({\"imgt.sequence_gapped_aa\": \"sequence_aa\"}, axis=1)\n",
    "    .set_index(\"gene\")[\"sequence_aa\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "human_lookup_j = (\n",
    "    j_genes_human[[\"gene\", \"imgt.sequence_gapped_aa\"]]\n",
    "    .rename({\"imgt.sequence_gapped_aa\": \"sequence_aa\"}, axis=1)\n",
    "    .set_index(\"gene\")[\"sequence_aa\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# for each human v, find the closest mac V based on the tables\n",
    "closest_ortholog_v = get_lookups(human_lookup_v, mac_lookup_v)\n",
    "closest_ortholog_j = get_lookups(human_lookup_j, mac_lookup_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5aa9134e-66f1-4c7f-98ff-351e1671d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put in linked airrtable the orthologs \n",
    "lat[\"v_ortholog_heavy\"] = lat[\"v_call_top_heavy\"].map(closest_ortholog_v)\n",
    "lat[\"v_ortholog_light\"] = lat[\"v_call_top_light\"].map(closest_ortholog_v)\n",
    "lat[\"j_ortholog\"] = lat[\"j_call_top_heavy\"].map(closest_ortholog_j)\n",
    "\n",
    "# where it's NA, put the word NONE\n",
    "lat.loc[lat[lat[\"v_ortholog_light\"].isna()].index, \"v_ortholog_light\"] = \"NONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9c7977c2-7465-4e91-8c6d-b10fb85a736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add HCDR3 Length\n",
    "lat[\"hcdr3_len\"] = lat[\"cdr3_aa_heavy\"].str.len()\n",
    "lat[\"lcdr3_len\"] = lat[\"cdr3_aa_light\"].str.len()\n",
    "\n",
    "# for any NA on CDR3 aa, just put length as 0. These will fall out on cluster\n",
    "lat.loc[lat[lat[\"cdr3_aa_heavy\"].isna()].index, \"hcdr3_len\"] = 0\n",
    "lat.loc[lat[lat[\"cdr3_aa_light\"].isna()].index, \"lcdr3_len\"] = 0\n",
    "\n",
    "# also put the family in the dataframe\n",
    "lat[\"v_ortholog_heavy_family\"] = lat[\"v_ortholog_heavy\"].str.split(\"-\").str.get(0)\n",
    "lat[\"v_ortholog_light_family\"] = lat[\"v_ortholog_light\"].str.split(\"-\").str.get(0)\n",
    "\n",
    "# assert we have no NA\n",
    "assert not lat[\"v_ortholog_heavy_family\"].isna().any()\n",
    "assert not lat[\"v_ortholog_light_family\"].isna().any()\n",
    "assert not lat[\"hcdr3_len\"].isna().any()\n",
    "assert not lat[\"lcdr3_len\"].isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04b663-4dc4-4813-b0e9-5de01d3158d1",
   "metadata": {},
   "source": [
    "# BG18 Type I Definitions\n",
    "\n",
    "For all HCDR3 >= 22, find `ITIFG[LV]VI[IT]` and up to 4 mutations from  `ITIFG[LV]VI[IT]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2d31a911-3cc1-4436-add3-8d6923258ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_multi_regex(seq):\n",
    "    '''This groups the regex into segments and put them into a list\n",
    "    \n",
    "    example: split_multi_regex(split_multi_regex(\"ITIFG[LV]VI[IT]\")\n",
    "    \n",
    "    >>>['I', 'T', 'I', 'F', 'G', '[LV]', 'V', 'I', '[IT]']\n",
    "    \n",
    "    '''\n",
    "    new_seq = []\n",
    "    inside = False\n",
    "    inside_seq = \"\"\n",
    "    for i in seq:\n",
    "\n",
    "        if i == \"[\":\n",
    "            inside = True\n",
    "            inside_seq += \"[\"\n",
    "            continue\n",
    "        if i == \"]\":\n",
    "            new_seq.append(inside_seq + \"]\")\n",
    "            inside = False\n",
    "            inside_seq = \"\"\n",
    "            continue\n",
    "        if inside:\n",
    "            inside_seq += i\n",
    "        else:\n",
    "            new_seq.append(i)\n",
    "    return new_seq\n",
    "\n",
    "\n",
    "def get_regex_combinations(regex, tolerance):\n",
    "    '''Get all regex combinations\n",
    "    \n",
    "    ex: get_regex_combinations(\"ITIFG[LV]VI[IT]\",4)\n",
    "    \n",
    "     'ITI.G[LV]VI[IT]',\n",
    "     'ITIF.[LV]VI[IT]',\n",
    "     'ITIFG.VI[IT]',\n",
    "     'ITIFG[LV].I[IT]',\n",
    "     'ITIFG[LV]V.[IT]',\n",
    "     'ITIFG[LV]VI.',\n",
    "     'ITIFG[LV]VI[IT]',\n",
    "     'ITIFG[LV]VI[IT]'\n",
    "     ...\n",
    "    \n",
    "    '''\n",
    "    def mutate_base(positons):\n",
    "        base_seq = split_multi_regex(regex)\n",
    "        for x in positons:\n",
    "            base_seq[x] = \".\"\n",
    "        return \"\".join(base_seq)\n",
    "\n",
    "    regexes = []\n",
    "    for x in range(tolerance, -1, -1):\n",
    "        for y in list(combinations(range(len(split_multi_regex(regex))), x)):\n",
    "            regexes.append(mutate_base(list(y)))\n",
    "    regexes.append(regex)\n",
    "    return regexes\n",
    "\n",
    "\n",
    "def get_best_standard_def(df, column, regex, tolerance):\n",
    "    '''For every row in dataframe, find the best regex and best regex + '.E' and report it as a list of dictionaries\n",
    "    \n",
    "    ex:\n",
    "    >>>best_regex, best_regex_e = get_best_standard_def(\n",
    "        lat, \"cdr3_aa_heavy\", \"ITIFG[LV]VI[IT]\", 9\n",
    "    )\n",
    "    \n",
    "    >>>best_regex\n",
    "     {'index': 45608, 'best_regex': 'ITIFG[LV]VI.', 'tolerated_d_mutations': 1},\n",
    "     {'index': 45607, 'best_regex': 'ITIFG[LV]VI.', 'tolerated_d_mutations': 1},\n",
    "     {'index': 45606, 'best_regex': 'ITIFG[LV]VI.', 'tolerated_d_mutations': 1},\n",
    "    >>>best_regex_e\n",
    "     {'index': 34124,'best_regex_e': 'ITIFG[LV]VI[IT].E','tolerated_d_mutations': 0}\n",
    "    '''\n",
    "    matching_indexes = []\n",
    "    matching_indexes_w_e = []\n",
    "    regexes = get_regex_combinations(regex, tolerance)\n",
    "    for regex in regexes:\n",
    "        index = df[df[column].str.contains(regex, na=False)].index\n",
    "        e_regex = regex + \".E\"\n",
    "        e_index = df[df[column].str.contains(e_regex, na=False)].index\n",
    "        for i in index:\n",
    "            matching_indexes.append(\n",
    "                {\n",
    "                    \"index\": i,\n",
    "                    \"best_regex\": regex,\n",
    "                    \"tolerated_d_mutations\": regex.count(\".\"),\n",
    "                }\n",
    "            )\n",
    "        for i in e_index:\n",
    "            matching_indexes_w_e.append(\n",
    "                {\n",
    "                    \"index\": i,\n",
    "                    \"best_regex_e\": e_regex,\n",
    "                    \"tolerated_d_mutations\": regex.count(\".\"),\n",
    "                }\n",
    "            )\n",
    "    return matching_indexes, matching_indexes_w_e\n",
    "\n",
    "\n",
    "def find_regex_pos(X):\n",
    "    '''If you find a regex, find where it starts'''\n",
    "    r = re.search(str(X[1]), str(X[0]))\n",
    "    if r:\n",
    "        return r.start() + 1\n",
    "\n",
    "# find best regex and tolerated d mutations\n",
    "best_regex, best_regex_e = get_best_standard_def(\n",
    "    lat, \"cdr3_aa_heavy\", \"ITIFG[LV]VI[IT]\", 9\n",
    ")\n",
    "\n",
    "\n",
    "# best regex to df and get best tolerated d mutations\n",
    "best_regexs = (\n",
    "    pd.DataFrame(best_regex)\n",
    "    .sort_values(\"tolerated_d_mutations\")\n",
    "    .groupby(\"index\")\n",
    "    .head(1)\n",
    ")\n",
    "# best regex_e to df and get best tolerated d mutations\n",
    "best_e_regexes = (\n",
    "    pd.DataFrame(best_regex_e)\n",
    "    .sort_values(\"tolerated_d_mutations\")\n",
    "    .groupby(\"index\")\n",
    "    .head(1)\n",
    ")\n",
    "\n",
    "\n",
    "# add regexes to LinkedAirrTable\n",
    "all_combined_w_def = lat.join(best_regexs.set_index(\"index\"), how=\"left\")\n",
    "\n",
    "# add regexes with E to LinkedAirrTable\n",
    "all_combined_w_def = all_combined_w_def.join(\n",
    "    best_e_regexes.rename(\n",
    "        {\"tolerated_d_mutations\": \"tolerated_d_mutations_w_e\"}, axis=1\n",
    "    ).set_index(\"index\"),\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# find out where the best_regex_starts\n",
    "all_combined_w_def[\"best_regex_start\"] = all_combined_w_def[\n",
    "    [\"cdr3_aa_heavy\", \"best_regex\"]\n",
    "].apply(find_regex_pos, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e9df9c-65d3-4de9-9c64-ce1f1da93f20",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sadie-dev",
   "language": "python",
   "name": "sadie-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
