{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16617ca0-8003-424c-afc3-9614800c9657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "from sadie.airr.airrtable import LinkedAirrTable\n",
    "from sadie.reference import Reference\n",
    "from sadie.reference.yaml import YamlRef\n",
    "from Levenshtein import distance\n",
    "from itertools import combinations\n",
    "from sadie.airr.methods import run_mutational_analysis\n",
    "from sadie.cluster import Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f9d4cd-0339-492e-bb99-8b18164c6ce5",
   "metadata": {},
   "source": [
    "# Read in and pre-process data\n",
    "\n",
    "This is all 10X VDJ contigs and sorted cells in one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d52836f8-f112-408b-b344-0df545e0c8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in combined dataframe\n",
    "all_combined = pd.read_feather(\"../data/all_processed_combined_personalized.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c29ffda-7e82-4aed-a70f-0253709bcbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to to a LinkedAirrTable (a heavy-light paired airrtable)\n",
    "lat = LinkedAirrTable(all_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f097e1f-b7c6-4267-bacc-ef2e3e938751",
   "metadata": {},
   "source": [
    "## Add Closest Orthologs and CDR3 Length\n",
    "\n",
    "here we are taking the macaque gene and finding the closest human ortholog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f69b7109-cd2d-4d79-b905-40b275564e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lookups(human_lookup, mac_lookup):\n",
    "    '''A simple function to find the best scoring ortholog between human and macaque'''\n",
    "    lookups = {}\n",
    "    for x in mac_lookup:\n",
    "        lowest_score = 1000\n",
    "        for y in human_lookup:\n",
    "            d = distance(human_lookup[y], mac_lookup[x])\n",
    "            if d < lowest_score:\n",
    "                lookups[x] = y\n",
    "                lowest_score = d\n",
    "    return lookups\n",
    "\n",
    "# enter no file to use reference.yml\n",
    "yml_ref = YamlRef()\n",
    "\n",
    "# create empty reference object\n",
    "ref_class = Reference()\n",
    "genes = yml_ref[\"human\"][\"imgt\"][\"human\"]\n",
    "ref_class.add_genes(**{\"genes\": genes, \"species\": \"human\", \"source\": \"imgt\"})\n",
    "\n",
    "# make human\n",
    "human_df = ref_class.get_dataframe()\n",
    "v_genes_human = human_df.query(\"gene_segment=='V'\")\n",
    "d_genes_human = human_df.query(\"gene_segment=='D'\")\n",
    "j_genes_human = human_df.query(\"gene_segment=='J'\")\n",
    "\n",
    "\n",
    "# create empty reference object\n",
    "ref_class = Reference()\n",
    "genes = yml_ref[\"macaque\"][\"custom\"][\"macaque\"]\n",
    "ref_class.add_genes(**{\"genes\": genes, \"species\": \"macaque\", \"source\": \"custom\"})\n",
    "\n",
    "# make macaque ref\n",
    "macaque_df = ref_class.get_dataframe()\n",
    "v_genes_macaque = macaque_df.query(\"gene_segment=='V'\")\n",
    "d_genes_macaque = macaque_df.query(\"gene_segment=='D'\")\n",
    "j_genes_macaque = macaque_df.query(\"gene_segment=='J'\")\n",
    "\n",
    "# now make a lookup table for macaque and human genes. \n",
    "mac_lookup_v = (\n",
    "    v_genes_macaque.query(\"gene_segment=='V'\")\n",
    "    .query(\"`imgt.contrived_functional`=='F'\")[[\"gene\", \"imgt.sequence_gapped_aa\"]]\n",
    "    .rename({\"imgt.sequence_gapped_aa\": \"sequence_aa\"}, axis=1)\n",
    "    .set_index(\"gene\")[\"sequence_aa\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "human_lookup_v = (\n",
    "    v_genes_human.query(\"gene_segment=='V'\")\n",
    "    .query(\"`imgt.contrived_functional`=='F'\")[[\"gene\", \"imgt.sequence_gapped_aa\"]]\n",
    "    .rename({\"imgt.sequence_gapped_aa\": \"sequence_aa\"}, axis=1)\n",
    "    .set_index(\"gene\")[\"sequence_aa\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# do the same for the J gene\n",
    "mac_lookup_j = (\n",
    "    j_genes_macaque.query(\"gene_segment=='J'\")[[\"gene\", \"imgt.sequence_gapped_aa\"]]\n",
    "    .rename({\"imgt.sequence_gapped_aa\": \"sequence_aa\"}, axis=1)\n",
    "    .set_index(\"gene\")[\"sequence_aa\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "human_lookup_j = (\n",
    "    j_genes_human[[\"gene\", \"imgt.sequence_gapped_aa\"]]\n",
    "    .rename({\"imgt.sequence_gapped_aa\": \"sequence_aa\"}, axis=1)\n",
    "    .set_index(\"gene\")[\"sequence_aa\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# for each human v, find the closest mac V based on the tables\n",
    "closest_ortholog_v = get_lookups(human_lookup_v, mac_lookup_v)\n",
    "closest_ortholog_j = get_lookups(human_lookup_j, mac_lookup_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aa9134e-66f1-4c7f-98ff-351e1671d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put in linked airrtable the orthologs \n",
    "lat[\"v_ortholog_heavy\"] = lat[\"v_call_top_heavy\"].map(closest_ortholog_v)\n",
    "lat[\"v_ortholog_light\"] = lat[\"v_call_top_light\"].map(closest_ortholog_v)\n",
    "lat[\"j_ortholog\"] = lat[\"j_call_top_heavy\"].map(closest_ortholog_j)\n",
    "\n",
    "# where it's NA, put the word NONE\n",
    "lat.loc[lat[lat[\"v_ortholog_light\"].isna()].index, \"v_ortholog_light\"] = \"NONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c7977c2-7465-4e91-8c6d-b10fb85a736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add HCDR3 Length\n",
    "lat[\"hcdr3_len\"] = lat[\"cdr3_aa_heavy\"].str.len()\n",
    "lat[\"lcdr3_len\"] = lat[\"cdr3_aa_light\"].str.len()\n",
    "\n",
    "# for any NA on CDR3 aa, just put length as 0. These will fall out on cluster\n",
    "lat.loc[lat[lat[\"cdr3_aa_heavy\"].isna()].index, \"hcdr3_len\"] = 0\n",
    "lat.loc[lat[lat[\"cdr3_aa_light\"].isna()].index, \"lcdr3_len\"] = 0\n",
    "\n",
    "# also put the family in the dataframe\n",
    "lat[\"v_ortholog_heavy_family\"] = lat[\"v_ortholog_heavy\"].str.split(\"-\").str.get(0)\n",
    "lat[\"v_ortholog_light_family\"] = lat[\"v_ortholog_light\"].str.split(\"-\").str.get(0)\n",
    "\n",
    "# assert we have no NA\n",
    "assert not lat[\"v_ortholog_heavy_family\"].isna().any()\n",
    "assert not lat[\"v_ortholog_light_family\"].isna().any()\n",
    "assert not lat[\"hcdr3_len\"].isna().any()\n",
    "assert not lat[\"lcdr3_len\"].isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04b663-4dc4-4813-b0e9-5de01d3158d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Annotate BG18 Type I Criteria\n",
    "\n",
    "For all HCDR3 >= 22, find `ITIFG[LV]VI[IT]` and up to 4 mutations from  `ITIFG[LV]VI[IT]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d31a911-3cc1-4436-add3-8d6923258ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_multi_regex(seq):\n",
    "    '''This groups the regex into segments and put them into a list\n",
    "    \n",
    "    example: split_multi_regex(split_multi_regex(\"ITIFG[LV]VI[IT]\")\n",
    "    \n",
    "    >>>['I', 'T', 'I', 'F', 'G', '[LV]', 'V', 'I', '[IT]']\n",
    "    \n",
    "    '''\n",
    "    new_seq = []\n",
    "    inside = False\n",
    "    inside_seq = \"\"\n",
    "    for i in seq:\n",
    "\n",
    "        if i == \"[\":\n",
    "            inside = True\n",
    "            inside_seq += \"[\"\n",
    "            continue\n",
    "        if i == \"]\":\n",
    "            new_seq.append(inside_seq + \"]\")\n",
    "            inside = False\n",
    "            inside_seq = \"\"\n",
    "            continue\n",
    "        if inside:\n",
    "            inside_seq += i\n",
    "        else:\n",
    "            new_seq.append(i)\n",
    "    return new_seq\n",
    "\n",
    "\n",
    "def get_regex_combinations(regex, tolerance):\n",
    "    '''Get all regex combinations\n",
    "    \n",
    "    ex: get_regex_combinations(\"ITIFG[LV]VI[IT]\",4)\n",
    "    \n",
    "     'ITI.G[LV]VI[IT]',\n",
    "     'ITIF.[LV]VI[IT]',\n",
    "     'ITIFG.VI[IT]',\n",
    "     'ITIFG[LV].I[IT]',\n",
    "     'ITIFG[LV]V.[IT]',\n",
    "     'ITIFG[LV]VI.',\n",
    "     'ITIFG[LV]VI[IT]',\n",
    "     'ITIFG[LV]VI[IT]'\n",
    "     ...\n",
    "    \n",
    "    '''\n",
    "    def mutate_base(positons):\n",
    "        base_seq = split_multi_regex(regex)\n",
    "        for x in positons:\n",
    "            base_seq[x] = \".\"\n",
    "        return \"\".join(base_seq)\n",
    "\n",
    "    regexes = []\n",
    "    for x in range(tolerance, -1, -1):\n",
    "        for y in list(combinations(range(len(split_multi_regex(regex))), x)):\n",
    "            regexes.append(mutate_base(list(y)))\n",
    "    regexes.append(regex)\n",
    "    return regexes\n",
    "\n",
    "\n",
    "def get_best_standard_def(df, column, regex, tolerance):\n",
    "    '''For every row in dataframe, find the best regex and best regex + '.E' and report it as a list of dictionaries\n",
    "    \n",
    "    ex:\n",
    "    >>>best_regex, best_regex_e = get_best_standard_def(\n",
    "        lat, \"cdr3_aa_heavy\", \"ITIFG[LV]VI[IT]\", 9\n",
    "    )\n",
    "    \n",
    "    >>>best_regex\n",
    "     {'index': 45608, 'best_regex': 'ITIFG[LV]VI.', 'tolerated_d_mutations': 1},\n",
    "     {'index': 45607, 'best_regex': 'ITIFG[LV]VI.', 'tolerated_d_mutations': 1},\n",
    "     {'index': 45606, 'best_regex': 'ITIFG[LV]VI.', 'tolerated_d_mutations': 1},\n",
    "    >>>best_regex_e\n",
    "     {'index': 34124,'best_regex_e': 'ITIFG[LV]VI[IT].E','tolerated_d_mutations': 0}\n",
    "    '''\n",
    "    matching_indexes = []\n",
    "    matching_indexes_w_e = []\n",
    "    regexes = get_regex_combinations(regex, tolerance)\n",
    "    for regex in regexes:\n",
    "        index = df[df[column].str.contains(regex, na=False)].index\n",
    "        e_regex = regex + \".E\"\n",
    "        e_index = df[df[column].str.contains(e_regex, na=False)].index\n",
    "        for i in index:\n",
    "            matching_indexes.append(\n",
    "                {\n",
    "                    \"index\": i,\n",
    "                    \"best_regex\": regex,\n",
    "                    \"tolerated_d_mutations\": regex.count(\".\"),\n",
    "                }\n",
    "            )\n",
    "        for i in e_index:\n",
    "            matching_indexes_w_e.append(\n",
    "                {\n",
    "                    \"index\": i,\n",
    "                    \"best_regex_e\": e_regex,\n",
    "                    \"tolerated_d_mutations\": regex.count(\".\"),\n",
    "                }\n",
    "            )\n",
    "    return matching_indexes, matching_indexes_w_e\n",
    "\n",
    "\n",
    "def find_regex_pos(X):\n",
    "    '''If you find a regex, find where it starts'''\n",
    "    r = re.search(str(X[1]), str(X[0]))\n",
    "    if r:\n",
    "        return r.start() + 1\n",
    "\n",
    "# find best regex and tolerated d mutations\n",
    "best_regex, best_regex_e = get_best_standard_def(\n",
    "    lat, \"cdr3_aa_heavy\", \"ITIFG[LV]VI[IT]\", 9\n",
    ")\n",
    "\n",
    "\n",
    "# best regex to df and get best tolerated d mutations\n",
    "best_regexs = (\n",
    "    pd.DataFrame(best_regex)\n",
    "    .sort_values(\"tolerated_d_mutations\")\n",
    "    .groupby(\"index\")\n",
    "    .head(1)\n",
    ")\n",
    "# best regex_e to df and get best tolerated d mutations\n",
    "best_e_regexes = (\n",
    "    pd.DataFrame(best_regex_e)\n",
    "    .sort_values(\"tolerated_d_mutations\")\n",
    "    .groupby(\"index\")\n",
    "    .head(1)\n",
    ")\n",
    "\n",
    "\n",
    "# add regexes to LinkedAirrTable\n",
    "all_combined_w_def = lat.join(best_regexs.set_index(\"index\"), how=\"left\")\n",
    "\n",
    "# add regexes with E to LinkedAirrTable\n",
    "all_combined_w_def = all_combined_w_def.join(\n",
    "    best_e_regexes.rename(\n",
    "        {\"tolerated_d_mutations\": \"tolerated_d_mutations_w_e\"}, axis=1\n",
    "    ).set_index(\"index\"),\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# find out where the best_regex_starts\n",
    "all_combined_w_def[\"best_regex_start\"] = all_combined_w_def[\n",
    "    [\"cdr3_aa_heavy\", \"best_regex\"]\n",
    "].apply(find_regex_pos, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e9df9c-65d3-4de9-9c64-ce1f1da93f20",
   "metadata": {},
   "source": [
    "# Annotate BG18 Type I Alternate Criteria\n",
    "\n",
    "For all HCDR3 >= 22, use this '[WFY]G' + 'VLQFLEWLLY' and tolerate 4 mutations only in the VLQFLEWLLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ca74250-e021-43bf-a44f-f4bda74e5b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_alternate_def(df, column, regex, tolerance):\n",
    "    matching_indexes = []\n",
    "    matching_indexes_w_e = []\n",
    "    regexes = get_regex_combinations(regex, tolerance)\n",
    "    for regex in regexes:\n",
    "        index = df[df[column].str.contains(regex, na=False)].index\n",
    "        e_regex = \"[WFY]G\" + regex\n",
    "        e_index = df[df[column].str.contains(e_regex, na=False)].index\n",
    "        for i in index:\n",
    "            matching_indexes.append(\n",
    "                {\n",
    "                    \"index\": i,\n",
    "                    \"best_regex_alternate\": regex,\n",
    "                    \"tolerated_d_mutations_alternate\": regex.count(\".\"),\n",
    "                }\n",
    "            )\n",
    "        for i in e_index:\n",
    "            matching_indexes_w_e.append(\n",
    "                {\n",
    "                    \"index\": i,\n",
    "                    \"best_regex_alternate_prepend\": e_regex,\n",
    "                    \"tolerated_d_mutations_alternate_prepend\": regex.count(\".\"),\n",
    "                }\n",
    "            )\n",
    "    return matching_indexes, matching_indexes_w_e\n",
    "\n",
    "\n",
    "# get best alternate def\n",
    "best_regex, best_regex_e = get_best_alternate_def(\n",
    "    all_combined_w_def, \"cdr3_aa_heavy\", \"VLQFLEWLLY\", 10\n",
    ")\n",
    "\n",
    "# best regexs\n",
    "best_regexs = (\n",
    "    pd.DataFrame(best_regex)\n",
    "    .sort_values(\"tolerated_d_mutations_alternate\")\n",
    "    .groupby(\"index\")\n",
    "    .head(1)\n",
    ")\n",
    "\n",
    "# best regexes with a prepend [WFY]G\n",
    "best_prepend_regexes = (\n",
    "    pd.DataFrame(best_regex_e)\n",
    "    .sort_values(\"tolerated_d_mutations_alternate_prepend\")\n",
    "    .groupby(\"index\")\n",
    "    .head(1)\n",
    ")\n",
    "\n",
    "# join on dataframe\n",
    "all_combined_w_alternate = all_combined_w_def.join(\n",
    "    best_regexs.set_index(\"index\"), how=\"left\"\n",
    ")\n",
    "\n",
    "# join prepend on dataframe\n",
    "all_combined_w_alternate = all_combined_w_alternate.join(\n",
    "    best_prepend_regexes.set_index(\"index\"),\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# find out where those regexs start\n",
    "all_combined_w_alternate[\"best_regex_start_alternate\"] = all_combined_w_alternate[\n",
    "    [\"cdr3_aa_heavy\", \"best_regex_alternate\"]\n",
    "].apply(find_regex_pos, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa45e05-1d6c-44cf-af0e-71bcac63d162",
   "metadata": {},
   "source": [
    "# Assign Precursor Definitions\n",
    "\n",
    "Now that we have the criteria including the best regexes and how many mutations are in the D gene regex, we can look through all the fields and determine if a sequence is a BG18 precursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cee7d583-fd9c-485e-98fc-177d5dac7480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex must be in the 4,5 or 6th position\n",
    "# it must also have the best regex with less than 4 mutations\n",
    "pos = [4, 5, 6]\n",
    "has_bg18 = (\n",
    "    all_combined_w_alternate.query(\"hcdr3_len>21\")\n",
    "    .query(\"tolerated_d_mutations_w_e<5\")\n",
    "    .query(\"best_regex_start in @pos\")\n",
    "    .index\n",
    ")\n",
    "\n",
    "# We don't care what position the alt is in, but must have prepend <5\n",
    "has_bg18_alt = (\n",
    "    all_combined_w_alternate.query(\"hcdr3_len>21\")\n",
    "    .query(\"tolerated_d_mutations_alternate_prepend < 5\")\n",
    "    .index\n",
    ")\n",
    "\n",
    "# first assign everything to false\n",
    "all_combined_w_alternate[\"is_bg18_precursor\"] = False\n",
    "all_combined_w_alternate[\"is_bg18_alt_precursor\"] = False\n",
    "\n",
    "# use the indexes to update what lines are precursors\n",
    "all_combined_w_alternate.loc[has_bg18, \"is_bg18_precursor\"] = True\n",
    "all_combined_w_alternate.loc[has_bg18_alt, \"is_bg18_alt_precursor\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8389489-d3ea-4e55-aa59-0ae8ba7047e1",
   "metadata": {},
   "source": [
    "# Run Mutational Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0911912f-f6a4-4bba-8000-37f847b76de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn all_combined_w_alternate into a linked airr table and reset the index so its a 1->N as a field in the dataframe\n",
    "lat = LinkedAirrTable(all_combined_w_alternate.reset_index(),key_column='index')\n",
    "\n",
    "# run the mutational analysis\n",
    "lat_with_mt = run_mutational_analysis(lat,'kabat')\n",
    "\n",
    "# get the mutations heavy and light out of the table \n",
    "lat_with_mt = lat_with_mt[['index','mutations_heavy','mutations_light']]\n",
    "\n",
    "# turn index to int because SADIE turns it to a string\n",
    "lat_with_mt['index'] = lat_with_mt['index'].astype(int)\n",
    "\n",
    "# join lat_with_mt back on lat\n",
    "lat = lat.merge(lat_with_mt[['index','mutations_heavy','mutations_light']],on='index',how='left').drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0cb5ef-3764-4bad-9b01-806e7fb6bc64",
   "metadata": {},
   "source": [
    "# Run Clustering of BG18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00bf55ed-e25b-495e-aea9-2f5ffc94b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all precursors out\n",
    "bg18_precursors = lat.query(\"is_bg18_precursor or is_bg18_alt_precursor\").copy()\n",
    "\n",
    "# change to linked airr table\n",
    "lat_bg18 = LinkedAirrTable(bg18_precursors)\n",
    "\n",
    "# make a cluster api with average linkage\n",
    "cluster_api = Cluster(lat_bg18,linkage='average',groupby=['NHP','hcdr3_len'],lookup=['cdr1_heavy','cdr2_heavy','cdr3_heavy'])\n",
    "\n",
    "\n",
    "#cluster with distance of 3 but catch the stupid depreciation warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    lat_bg18_w_cluster = cluster_api.cluster(3)\n",
    "\n",
    "# rejoin on self\n",
    "lat_w_cluster = lat.join(lat_bg18_w_cluster['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b073171d-9470-4170-8d7f-b9bc88eaffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# presto, everything you need\n",
    "lat_w_cluster.to_feather(\"../data/all_processed_combined_personalized_w_bg18.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ae2a171-c3d9-4c61-bae1-29b5b1103df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_w_cluster.to_csv(\"../data/all_processed_combined_personalized_w_bg18.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeacb91-97ba-4c84-8275-8dbe4850127c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sadie-dev",
   "language": "python",
   "name": "sadie-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
